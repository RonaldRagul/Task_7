# -*- coding: utf-8 -*-
"""DataPreprocessing_Assignment11 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pl7yQlevQ3obRwEfNbRv3J-bXKaagMP9
"""

#Data.csv

"""**Step 1: Importing the libraries**"""

from google.colab import drive
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

"""**Step 2: Importing dataset**"""

drive.mount('/content/drive')
os.chdir('/content/drive/My Drive/Task_7')
df = pd.read_csv('Data.csv')
df.head()

"""**Step 3: Handling the missing data**"""

df.info
df.describe()
df.isnull().sum()

df['Age'] = df['Age'].fillna(df['Age'].mean())
df['Salary'] = df['Salary'].fillna(df['Salary'].mean())

"""**Step 4: Encoding categorical data**"""

le = preprocessing.LabelEncoder()

df['Country'] =le.fit_transform(df['Country'])
df['Purchased'] =le.fit_transform(df['Purchased'])

"""**Step 5: Creating a dummy variable**"""

dummy =pd.get_dummies(df['Purchased'])
df = pd.concat([df, dummy], axis = 1)

"""**Step 6: Splitting the datasets into training sets and Test sets**"""

x =df.drop('Purchased',axis =1)
y =df.Purchased

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=10)

x_train.shape
y_train.shape

"""**Step 7: Feature Scaling**"""

sc= StandardScaler() # mean = 0 and deviation = 1,
sc.fit(x_train)
x_train= sc.transform(x_train)
sc.fit(x_test)
x_test= sc.transform(x_test)
x.shape